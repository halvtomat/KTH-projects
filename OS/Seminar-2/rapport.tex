\documentclass[11pt]{article}

\title{Next, previous, before and after}
\date{2020-01-13}
\author{Daniel Gustafsson}

\begin{document}

\maketitle
\pagenumbering{gobble}

\newpage
\pagenumbering{arabic}

\paragraph{Abstract}
Improvement focus was on making overhead as small as possible. Some performance-costly measures were taken. 
When managing a number of random-sized memory requests, performance is between 1-11\% of that of built in malloc.
When instead managing fixed-size requests performance is between 20-25\% better than that of built in malloc.

\section{Introduction}

\paragraph{}
Everyone who has ever tried programming in C should have heard of "malloc" and "free", these two functions provide a way for the programmer to easily
allocate memory on the heap of a process. Modern day memory management is fast and efficient, but what acctually goes on under the hood of the
operating system when a process calls "malloc"? In this report I will create my own heap manager similar to "malloc" and explore different aspects 
approaches, can i make it really space-efficient? Will it have a speed payoff? Continue reading and you will find answers.

\section{Background}

\paragraph{}
To create your own heap manager, you first need to understand how a heap manager works. The traditional way is to have a linked list data-structure 
where you keep all the free blocks from your heap, then when the process wants to allocate something the manager finds a free block with appropriate
size and returns the virtual memory address. Later when (hopefully) "free" is called by the process, the manager will put the newly freed block 
back in the list of free blocks. To make this possible we need to implement a couple things. Firstly we need to get memory from the operating system
, we can do this with the "mmap" system call, we should get as much memory as we think the process will use right from the start because 
calling the system to get more all the time would be very performance costly. Now we need a header with some information to be stored in every block
on the heap. To start, the header should contain the following information,
\begin{itemize}
    \item Free - if the block is free or taken
    \item Size - the size of the block
    \item Bfree - if the block before is free or taken
    \item Bsize - the size of the block before
    \item Next - pointer to the next block in the list
    \item Prev - pointer to the previous block in the list
\end{itemize}

\paragraph{}
When a block is requested, the manager needs to make sure the block is of appropriatesize, neither too small nor too big. A block that is too small would
quickly result in a segmentation fault, if the manager keeps giving out blocks that are too big the memory will run out (internal fragmentation). To solve this the manager tries
always looks for a block that is bigger than the one requested and then if it is too big split it into two smaller blocks. This creates a new problem,
after a while all blocks will be really small becuase of the splitting (external fragmentation) to solve this the manager will try to merge neighboring 
blocks when they are freed. This is the reason for each block to contain information about the block before it, to get the block after the current one in
memory, just check the block that is 'sizeof(header) + size' after it. To get the block before, check the address that is '-(sizeof(header) + Bsize)' before it.


\section{Method}

I focused my improvements on lowering overhead, I did this using 3 different improvement methods.

\paragraph{}
The first improvement I made on lowering overhead was removing the "Prev"-pointer, this would lower overhead by 8 bytes therefore making it a big
first improvement. The tradeof of this is that the Free-list is no longer a double linked list but a single linked list, this has performance
drawbacks on "detach" and "insert" operations on the list. When detaching, the manager now have to change just one pointer, this pointer is the 
"Next"-pointer of the previous item on the free-list, this is problematic because the manager has no easy way of finding the previous item like
it did before. The manager now has to go through the whole list to find the item and this can become performance-costly when the list is long.

\paragraph{}
The second improvement I made was reducing the "Next"-pointer from 8 to 4 bytes. I achieved this by removing the 4 most significant bytes
of the pointer, making it a 32bit int instead. This is possible because I know that this heap will never grow beyond a 32bit address-space
and therefore will always have teh same 4 most significant bytes. The implementation of this is quite simple, two new functions are 
introduced, ADRS and NEXT. ADRS returns a unsigned 32bit integer, this integer is calculated using by subtracting the address to the arena
(the start of the heap-space) from a block. This will be the 32bit (4 byte) address of the block and is what will be used in another blocks
"Next"-pointer. The NEXT function returns a pointer to a head structure, similarly this pointer address is calculated by taken the 
unsigned 32bit int from ADRS and adding the address to the arena. 

This second improvement can also have significant performance tradeofs, especially in combination with the first improvement. Now each time
the manager want to traverse the free-list, it needs to convert every "pointer"(32bit unsigned int) to the real address with NEXT function.
This isn't a big problem when the list is short.

\paragraph{}
The third and final improvement is the removal of "Free" and "Bfree". The information those two variables conatined is important for the
functionality of the manager, but only 1 bit in each of the 16bit variables is used. I realized that both "Size" and "Bsize" are aligned 
to 8, this means that the 3 least significant bits of those variables are unused. After these realizations the LSB of "Size" is the 
"Free"-flag and the LSB of "Bsize" is the "Bfree"-flag, this reduced the overhead a further 4 bytes.

This is my personal favorite of the improvements since it was really easy to implement, I just made the manager mask the bits of "Size"
and "Bsize" to get either the size or the free-flags. Another great thing about this is that (to my knowledge) there are no performance
tradeofs.

\section{Results}

\paragraph{}
Results were mixed, as expected. Mainly two benchmarks were used, both very similar and heavily influated by Assignment 5 from the operating systems course.
Both benchmarks ran 100 rounds of 10000 memory requests, with a size 50-500 buffer that , with the help of a random number generator, frees up some of the 
requested memory everytime a request is made. Every round meassures the time the manager takes to finish, than when all rounds are finished prints out the 
average time it took to complete a round. The benchmark also tests the original "dlmall" we made and the built in malloc function, than compares the results
of the timers for us to se the relative performance. 
The difference between the two benchmarks is that one request random-size memory and one request fixed-size memory, there are significant differences 
between the two.

\begin{verbatim}
Results from random-size requests:

random size         buffer 500
Dalloc_2.0 average time used:   0.480701  diff:   100.0%
Dalloc average time used:       0.013613  diff:   2.8%
Malloc average time used:       0.007182  diff:   1.5%

random size         buffer 50
Dalloc_2.0 average time used:   0.061236  diff:   100.0%
Dalloc average time used:       0.013666  diff:   22.3%
Malloc average time used:       0.007299  diff:   11.9%
\end{verbatim}

\begin{verbatim}
Results from fixed-size requests:

fixed size 16byte   buffer 500
Dalloc_2.0 average time used:   0.005864  diff:   100.0%
Dalloc average time used:       0.014031  diff:   239.3%
Malloc average time used:       0.007322  diff:   124.9%

fixed size 16byte   buffer 50
Dalloc_2.0 average time used:   0.005825  diff:   100.0%
Dalloc average time used:       0.013827  diff:   237.4%
Malloc average time used:       0.007309  diff:   125.5%

fixed size 128byte  buffer 50
Dalloc_2.0 average time used:   0.005833  diff:   100.0%
Dalloc average time used:       0.013702  diff:   234.9%
Malloc average time used:       0.007048  diff:   120.8%

fixed size 64byte   buffer 500
Dalloc_2.0 average time used:   0.005918  diff:   100.0%
Dalloc average time used:       0.013708  diff:   231.6%
Malloc average time used:       0.007217  diff:   122.0%
\end{verbatim}

\paragraph{}
As we can see the performance is less than satisfactory in a random-size memory request environment, and as predicted a long free-list is 
badly handled by the improved memory manager. The reduced buffer size in the second test improved performance x10 but was still almost
10x slower than built in malloc and even 5x slower than the original "dlmall". The fixed-size benchmarks show completely different results.
When the size was fixed it seems as the other variables didn't matter anymore, very similar results across all the fixed-size benchmarks.

\end{document}